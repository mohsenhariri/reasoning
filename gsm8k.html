<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Evaluating LLMs using GSM8K Dataset</title>

    <meta name="description" content="A presentation on evaluating LLMs using GSM8K dataset">
    <meta name="author" content="Mohsen Hariri">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Include Reveal.js stylesheets -->
    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="dist/theme/black.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css">
</head>

<body>

    <div class="reveal">

        <!-- Slides will go here -->
        <div class="slides">

            <!-- Title Slide -->
            <section>
                <h1>Evaluating LLMs using GSM8K Dataset</h1>
                <h3>Hands-on Workshop</h3>
                <!-- <p>Your Name</p> -->
                <!-- <p>Date</p> -->
            </section>

            <!-- Introduction -->
            <section>
                <h2>Introduction</h2>
                <ul>
                    <li> We will evaluate LLMs using the GSM8K dataset. </li>
                    <li> We will walk through the code and show how you can use it to run evaluations yourself. </li>
                    <li> The code is available <a href="https://github.com/mohsenhariri/llm/blob/main/llm/evaluate/gsm8k.py" target="_blank">here</a>. </li>
                    <li> You can find the CoT template <a href="https://github.com/mohsenhariri/llm/tree/main/templates" target="_blank">here</a>. </li>
                </ul>
            </section>
            


            <!-- Setup and Imports -->
            <section>
                <h2>Setup and Imports</h2>
                <pre><code class="language-python">
import argparse
import csv
import json
import logging
from datetime import datetime
from pathlib import Path

import torch
from datasets import concatenate_datasets, load_dataset
from evaluate.gsm8k_parse_ans import clean_response
from helpers.profiling import profiler
from init import init
from transformers import AutoModelForCausalLM, AutoTokenizer
                </code></pre>
            </section>

            <!-- parse_args function -->
            <section>
                <h2><code>parse_args()</code> Function</h2>
                <p>This function parses command-line arguments.</p>
                <pre><code class="language-python">
def parse_args():
    parser = argparse.ArgumentParser(description="Checkpoints to evaluate")
    parser.add_argument(
        "--checkpoints",
        nargs="+",
        default=["meta-llama/Llama-2-7b-hf"],
        help="List of checkpoints to use",
    )
    return parser.parse_args()
                </code></pre>
            </section>

            <!-- create_cot_prompt function -->
            <section>
                <h2><code>create_cot_prompt()</code> Function</h2>
                <p>Creates the Chain-of-Thought (CoT) prompt used for few-shot learning.</p>
                <pre><code class="language-python">
def create_cot_prompt(
    json_file, n_shots=8, reasoning_trigger="", final_answer_trigger="", idle_trigger=""
):

    assert n_shots <= 8, "n_shots should be less than or equal to 8"

    questions = []
    chains = []
    answers = []

    with open(json_file, "r") as file:
        data = json.load(file)

    for item in data:
        questions.append(item["question"])
        chains.append(" ".join(item["reasoning"]))
        answers.append(item["answer"])

    questions, chains, answers = (
        questions[:n_shots],
        chains[:n_shots],
        answers[:n_shots],
    )

    cot_prompt = ""

    for i in range(n_shots):
        parts = []
        if reasoning_trigger:
            parts.append(reasoning_trigger)
        parts.append(chains[i])
        if final_answer_trigger:
            parts.append(final_answer_trigger)
        parts.append(answers[i])
        if idle_trigger:
            parts.append(idle_trigger)

        response = " ".join(parts)
        cot_prompt += f"Q: {questions[i]}\nA: {response}.\n\n"

    return cot_prompt
                </code></pre>
            </section>

            <!-- Explanation for create_cot_prompt -->
            <section>
                <h2>Understanding <code>create_cot_prompt()</code></h2>
                <ul>
                    <li>Loads example questions and answers from a JSON file.</li>
                    <li>Constructs a prompt by concatenating questions, reasoning chains, and answers.</li>
                    <li>Used for few-shot prompting to guide the LLM.</li>
                </ul>
            </section>

            <!-- load_gsm8k function -->
            <section>
                <h2><code>load_gsm8k()</code> Function</h2>
                <p>Loads the GSM8K dataset for evaluation.</p>
                <pre><code class="language-python">
def load_gsm8k(subset="test"):
    gsm = load_dataset("openai/gsm8k", "main")

    if subset == "train":
        return gsm["train"]
    elif subset == "test":
        return gsm["test"]
    elif subset == "both":
        gsm_eval = concatenate_datasets([gsm["train"], gsm["test"]])
        gsm_eval = gsm_eval.shuffle(seed=42)
        return gsm_eval
                </code></pre>
            </section>

            <!-- concat_cot_prompt function -->
            <section>
                <h2><code>concat_cot_prompt()</code> Function</h2>
                <p>Concatenates the CoT prompt with the question to form the final prompt.</p>
                <pre><code class="language-python">
def concat_cot_prompt(cot_prompt, question):
    prompt = cot_prompt + f"Q: {question}\nA:"
    return prompt
                </code></pre>
            </section>

            <!-- load_model function -->
            <section>
                <h2><code>load_model()</code> Function</h2>
                <p>Loads the pre-trained language model and tokenizer.</p>
                <pre><code class="language-python">
def load_model(checkpoint):

    model = AutoModelForCausalLM.from_pretrained(
        checkpoint, device_map="auto", torch_dtype=torch.float16
    )
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = (
            tokenizer.eos_token if tokenizer.eos_token is not None else "[PAD]"
        )

    if model.generation_config.pad_token_id is None:
        model.generation_config.pad_token_id = tokenizer.pad_token_id

    model.generation_config.temperature = None
    model.generation_config.top_p = None
    model.generation_config.top_k = None
    model.generation_config.penalty_alpha = None
    model.generation_config.do_sample = False

    model.eval()

    return model, tokenizer
                </code></pre>
            </section>

            <!-- Explanation for load_model -->
            <section>
                <h2>Understanding <code>load_model()</code></h2>
                <ul>
                    <li>Loads a pre-trained language model specified by the checkpoint.</li>
                    <li>Configures tokenizer and model generation settings.</li>
                    <li>Sets model to evaluation mode.</li>
                </ul>
            </section>

            <!-- generate_response function -->
            <section>
                <h2><code>generate_response()</code> Function</h2>
                <p>Generates a response from the model given a prompt.</p>
                <pre><code class="language-python">
def generate_response(
    prompt, model, tokenizer, generate_kwargs, remove_input_prompt=True
):
    input = tokenizer(
        prompt, padding=False, add_special_tokens=True, return_tensors="pt"
    ).to(model.device)
    output = model.generate(**input, **generate_kwargs)
    output = output[0]

    input_prompt_len = input["input_ids"].shape[1]

    if remove_input_prompt:
        output = output[input_prompt_len:]

    response = tokenizer.decode(output, skip_special_tokens=True)

    return response, input_prompt_len
                </code></pre>
            </section>

            <!-- evaluate function -->
            <section>
                <h2><code>evaluate()</code> Function</h2>
                <p>Evaluates the model's responses against the GSM8K dataset.</p>
                <pre><code class="language-python">
def evaluate(
    model,
    tokenizer,
    generate_kwargs,
    n_shots=8,
    subset="test",
    iterations=None,
    save_response=False,
):

    # Triggers
    # reasoning_trigger = "Let's think step by step."
    reasoning_trigger = "Let's break it down."

    final_answer_trigger = "The answer is"
    # final_answer_trigger = "Therefore, the final answer is"

    # idle_trigger = "Let's go to the next question"

    cot_prompt = create_cot_prompt(
        "templates/gsm8k_cot.json",
        n_shots=n_shots,
        reasoning_trigger=reasoning_trigger,
        final_answer_trigger=final_answer_trigger,
        # idle_trigger=idle_trigger,
    )

    readable_responses, corrects, input_length_total, input_length_avg = 0, 0, 0, 0.0

    gsm_eval = load_gsm8k(subset=subset)
    num_questions = len(gsm_eval)

    csv_file_path_successful = f"{save_response}/successful_responses.csv"
    csv_file_path_unsuccessful = f"{save_response}/unsuccessful_responses.csv"

    columns = ["index", "question", "resp", "resp_parsed", "ans", "ans_parsed"]

    with open(csv_file_path_successful, mode="a", newline="") as file:
        writer = csv.writer(file)
        writer.writerow(columns + ["correct"])
        writer.writerow([-1, cot_prompt, "NA", "NA", "NA", "NA", "NA"])

    with open(csv_file_path_unsuccessful, mode="a", newline="") as file:
        writer = csv.writer(file)
        writer.writerow(columns + ["error"])
        writer.writerow([-1, cot_prompt, "NA", "NA", "NA", "NA", "NA"])

    for i in range(num_questions) if iterations is None else range(iterations):

        question, answer_truth_ = gsm_eval[i]["question"], gsm_eval[i]["answer"]
        answer_truth = answer_truth_.split(" ")[-1].replace(",", "")

        prompt = concat_cot_prompt(cot_prompt, question)

        print(f"Iteration, {i} \n")

        response, input_prompt_len = generate_response(
            prompt, model, tokenizer, generate_kwargs
        )

        final_answer_prediction = clean_response(response)

        if isinstance(final_answer_prediction, (int, float)):

            try:

                final_answer_truth = (
                    float(answer_truth) if "." in answer_truth else int(answer_truth)
                )

                if final_answer_prediction == final_answer_truth:
                    corrects += 1

                if save_response:
                    with open(csv_file_path_successful, mode="a", newline="") as file:
                        writer = csv.writer(file)

                        writer.writerow(
                            [
                                i,
                                question,
                                response,
                                final_answer_prediction,
                                answer_truth_,
                                final_answer_truth,
                                final_answer_prediction == final_answer_truth,
                            ]
                        )

                readable_responses += 1
                input_length_total += input_prompt_len

            except ValueError as e:
                print(f"Error: Value Error in iteration {i} \n")
                with open(csv_file_path_unsuccessful, mode="a", newline="") as file:
                    writer = csv.writer(file)
                    writer.writerow(
                        [
                            i,
                            question,
                            response,
                            final_answer_prediction,
                            answer_truth_,
                            answer_truth,
                            f"Cannot convert true answer, {e}",
                        ]
                    )

        else:
            print(f"Error: Value Error in iteration {i} \n")
            with open(csv_file_path_unsuccessful, mode="a", newline="") as file:
                writer = csv.writer(file)
                writer.writerow(
                    [
                        i,
                        question,
                        response,
                        final_answer_prediction,
                        answer_truth_,
                        answer_truth,
                        f"Error in parsing response",
                    ]
                )

    if readable_responses == 0:
        accuracy, input_length_avg = 0, 0

    else:
        accuracy = corrects / readable_responses
        input_length_avg = input_length_total / readable_responses

    print(f"Accuracy: {accuracy}")

    return accuracy, readable_responses, input_length_avg, num_questions
                </code></pre>
            </section>

            <!-- Explanation for evaluate function -->
            <section>
                <h2>Understanding <code>evaluate()</code></h2>
                <ul>
                    <li>Iterates over the GSM8K dataset and generates responses.</li>
                    <li>Parses the model's output and compares it to the true answer.</li>
                    <li>Logs results and calculates accuracy.</li>
                </ul>
            </section>

            <!-- evaluate_init function -->
            <section>
                <h2><code>evaluate_init()</code> Function</h2>
                <p>Initializes evaluation for a specific checkpoint.</p>
                <pre><code class="language-python">
def evaluate_init(checkpoint, seed=None):

    now = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

    checkpoint_path = checkpoint.replace("/", "_")
    output_dir = Path(rf"output/GSM/single_{checkpoint_path}_{now}")
    output_dir.mkdir(parents=True, exist_ok=True)

    generate_kwargs = {
        "num_return_sequences": 1,
        "max_new_tokens": 256,
        "do_sample": False,
        # "temperature": 0.6,
        # "top_k": 50,
    }
    n_shots = 8

    model, tokenizer = load_model(checkpoint)

    accuracy, readable_responses, input_length_avg, num_questions = evaluate(
        model,
        tokenizer,
        generate_kwargs,
        n_shots=n_shots,
        # subset="test",
        # iterations=None,
        # iterations=1,
        save_response=output_dir,
    )

    config = model.config
    config = config.to_dict()

    results = {
        "model": checkpoint,
        "accuracy": accuracy,
        "num_responses": readable_responses,
        "num_questions": num_questions,
        "num_shots": n_shots,
        "generate_kwargs": generate_kwargs,
        "config": config,
        "input_tokens_avg": input_length_avg,
        "seed": seed,
    }

    with open(output_dir / "results.json", "w") as f:
        json.dump(results, f)
                </code></pre>
            </section>

            <!-- main function -->
            <section>
                <h2><code>main()</code> Function</h2>
                <p>The main execution function that orchestrates the evaluation.</p>
                <pre><code class="language-python">
def main(seed=None):

    config = init() if seed is None else init(seed=seed)
    print(config.device)

    args = parse_args()
    checkpoints = args.checkpoints
    # checkpoints = [
    #     "meta-llama/Llama-2-7b-hf",
    #     "meta-llama/Llama-2-7b-chat-hf",
    #     "meta-llama/Meta-Llama-3-8B",
    #     "meta-llama/Meta-Llama-3-8B-Instruct",
    #     "mistralai/Mistral-7B-v0.1",
    #     "mistralai/Mistral-7B-Instruct-v0.1",
    #     "mistralai/Mistral-7B-Instruct-v0.2",
    #     "mistralai/Mistral-7B-Instruct-v0.3",
    #     "allenai/OLMo-1.7-7B-hf",
    # ]

    logging.basicConfig(
        filename="./logs/running.log",
        filemode="a",
        level=logging.ERROR,
        format="%(asctime)s %(levelname)s %(message)s",
    )

    for checkpoint in checkpoints:
        try:
            print(f"Running {checkpoint} \n")
            evaluate_init(checkpoint, seed=seed)
        except Exception as e:
            logging.error(f"Error: in {checkpoint} \n {e}")
        finally:
            torch.cuda.empty_cache()
                </code></pre>
            </section>

            <!-- The main block -->
            <section>
                <h2>The Main Block</h2>
                <pre><code class="language-python">
# Single run, profiling
if __name__ == "__main__":

    @profiler
    def profiling_main():
        return main()

    profile = profiling_main()

    print(profile)
                </code></pre>
            </section>

            <!-- Conclusion -->
        

            <section>
                <h2>What is Next?</h2>
                <ul>
                    <li> Understanding the code and running evaluations yourself. </li>
                    <li> Change the number of shots to see how it affects the model's performance. </li>
                    <li> What are the problems with the current evaluation code? </li>
                    <li> How can we improve it? </li>
                </ul>
            </section>


            <!-- Thank you slide -->
            <section>
                <h2>Thank You!</h2>
                <p>Questions?</p>
            </section>

        </div>

    </div>

    <!-- Include Reveal.js scripts -->
    <script src="dist/reveal.js"></script>
    <script src="plugin/zoom/zoom.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/search/search.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script>

        // Initialize Reveal.js
        Reveal.initialize({
            controls: true,
            progress: true,
            center: true,
            hash: true,

            // Learn about plugins: https://revealjs.com/plugins/
            plugins: [RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight]
        });

    </script>

</body>
</html>
